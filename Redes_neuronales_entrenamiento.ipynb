{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "prospective-america",
      "metadata": {
        "id": "prospective-america"
      },
      "source": [
        "Dado que el entrenamiento de redes neuronales es una tarea  muy costosa, **se recomienda ejecutar el notebooks en [Google Colab](https://colab.research.google.com)**, por supuesto también se puede ejecutar en local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prompt-developer",
      "metadata": {
        "id": "prompt-developer"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vocal-correction",
      "metadata": {
        "id": "vocal-correction"
      },
      "source": [
        "<a name='actividad_1'></a>\n",
        "# 1: Redes Densas\n",
        "\n",
        "Para este proyecto he utilizado el [wine quality dataset](https://archive.ics.uci.edu/ml/datasets/wine+quality). Con el que trataremos de predecir la calidad del vino.\n",
        "\n",
        "La calidad del vino puede tomar valores decimales (por ejemplo 7.25), independientemente de que en el dataset de entrenamiento sean números enteros. Por lo tanto, el problema es una `regresión`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "presidential-milan",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "presidential-milan",
        "outputId": "604c3d06-e01b-4bc6-8317-4deef925bf6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
              "0            7.4              0.70         0.00             1.9      0.076   \n",
              "1            7.8              0.88         0.00             2.6      0.098   \n",
              "2            7.8              0.76         0.04             2.3      0.092   \n",
              "3           11.2              0.28         0.56             1.9      0.075   \n",
              "4            7.4              0.70         0.00             1.9      0.076   \n",
              "\n",
              "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
              "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
              "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
              "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
              "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
              "\n",
              "   alcohol  quality  \n",
              "0      9.4        5  \n",
              "1      9.8        5  \n",
              "2      9.8        5  \n",
              "3      9.8        6  \n",
              "4      9.4        5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc2776cc-30df-45dc-a575-a6c11319d144\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.9970</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.9980</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc2776cc-30df-45dc-a575-a6c11319d144')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dc2776cc-30df-45dc-a575-a6c11319d144 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dc2776cc-30df-45dc-a575-a6c11319d144');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-30c4a1cd-5cc2-4a8e-a943-562921a83e40\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-30c4a1cd-5cc2-4a8e-a943-562921a83e40')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-30c4a1cd-5cc2-4a8e-a943-562921a83e40 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 6497,\n  \"fields\": [\n    {\n      \"column\": \"fixed acidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.296433757799792,\n        \"min\": 3.8,\n        \"max\": 15.9,\n        \"num_unique_values\": 106,\n        \"samples\": [\n          7.15,\n          8.1,\n          7.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"volatile acidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1646364740846772,\n        \"min\": 0.08,\n        \"max\": 1.58,\n        \"num_unique_values\": 187,\n        \"samples\": [\n          0.405,\n          0.21,\n          0.695\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"citric acid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14531786489759185,\n        \"min\": 0.0,\n        \"max\": 1.66,\n        \"num_unique_values\": 89,\n        \"samples\": [\n          0.1,\n          0.6,\n          0.37\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"residual sugar\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.757803743147445,\n        \"min\": 0.6,\n        \"max\": 65.8,\n        \"num_unique_values\": 316,\n        \"samples\": [\n          18.95,\n          3.2,\n          9.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chlorides\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03503360137245906,\n        \"min\": 0.009,\n        \"max\": 0.611,\n        \"num_unique_values\": 214,\n        \"samples\": [\n          0.089,\n          0.217,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"free sulfur dioxide\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.74939977200255,\n        \"min\": 1.0,\n        \"max\": 289.0,\n        \"num_unique_values\": 135,\n        \"samples\": [\n          77.5,\n          65.0,\n          128.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total sulfur dioxide\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56.521854522630264,\n        \"min\": 6.0,\n        \"max\": 440.0,\n        \"num_unique_values\": 276,\n        \"samples\": [\n          14.0,\n          149.0,\n          227.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"density\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002998673003719041,\n        \"min\": 0.98711,\n        \"max\": 1.03898,\n        \"num_unique_values\": 998,\n        \"samples\": [\n          0.9918,\n          0.99412,\n          0.99484\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pH\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1607872021039883,\n        \"min\": 2.72,\n        \"max\": 4.01,\n        \"num_unique_values\": 108,\n        \"samples\": [\n          3.74,\n          3.17,\n          3.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sulphates\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14880587361448958,\n        \"min\": 0.22,\n        \"max\": 2.0,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          1.11,\n          1.56,\n          0.46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"alcohol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.192711748870997,\n        \"min\": 8.0,\n        \"max\": 14.9,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          10.9333333333333,\n          9.7,\n          10.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quality\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 9,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          5,\n          6,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# Descargar los datos con pandas\n",
        "df_red = pd.read_csv('winequality-red.csv', sep=';')\n",
        "df_white = pd.read_csv('winequality-white.csv', sep=';')\n",
        "\n",
        "df = pd.concat([df_red, df_white])\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7c2f8aa",
      "metadata": {
        "id": "f7c2f8aa"
      },
      "outputs": [],
      "source": [
        "feature_names = [\n",
        "    'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides',\n",
        "    'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol'\n",
        "]\n",
        "\n",
        "# separar features y target\n",
        "y = df.pop('quality').values\n",
        "X = df.copy().values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f4adb77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f4adb77",
        "outputId": "45a41e80-7823-4828-d387-a0b04416e4e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train, y_train shapes: (4872, 11) (4872,)\n",
            "x_test, y_test shapes: (1625, 11) (1625,)\n",
            "Some qualities:  [6 7 8 5 6]\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "print('x_train, y_train shapes:', x_train.shape, y_train.shape)\n",
        "print('x_test, y_test shapes:', x_test.shape, y_test.shape)\n",
        "print('Some qualities: ', y_train[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "painted-extreme",
      "metadata": {
        "id": "painted-extreme"
      },
      "outputs": [],
      "source": [
        "## Normalizo las features.\n",
        "\n",
        "# Creo una instancia del escalador StandardScaler.\n",
        "# Este escalador transforma los datos para que tengan media 0 y desviación estándar 1.\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Ajusto el escalador a los datos de entrenamiento y transforma esos datos.\n",
        "# Esto me asegura que el modelo vea características con la misma escala.\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "\n",
        "# Uso el mismo escalador (ya ajustado con los datos de entrenamiento) para transformar los datos de test.\n",
        "x_test = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "underlying-planner",
      "metadata": {
        "id": "underlying-planner"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "## Creo un modelo secuencial que contenga 4 capas ocultas(hidden layers), con más de 60 neuronas  por capa, sin regularización y obtenga los resultados."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Fijo la semilla para NumPy, para asegurar resultados reproducibles.\n",
        "np.random.seed(42)\n",
        "\n",
        "# Fijo la semilla para TensorFlow, para asegurar que los resultados del entrenamiento sean consistentes.\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Defino el modelo secuencial con una capa de entrada explícita.\n",
        "# Con ello, mejoro la legibilidad y evito advertencias sobre input_shape.\n",
        "model = Sequential([\n",
        "    Input(shape=(x_train.shape[1],)),  # Capa de entrada, que se adapta al número de características de los datos.\n",
        "\n",
        "    # Creo la primera capa oculta, con 64 neuronas y activación ReLU.\n",
        "    # Uso la función da activación ReLU, ya que introduce no linealidades, es eficiente computacionalmente\n",
        "    # y ayuda a mitigar el problema del gradiente desvanecido.\n",
        "    Dense(64, activation='relu'),\n",
        "\n",
        "    # Creo la segunda capa oculta, idéntica a la primera.\n",
        "    Dense(64, activation='relu'),\n",
        "\n",
        "    # Creo la tercera capa oculta, con las mismas características.\n",
        "    Dense(64, activation='relu'),\n",
        "\n",
        "    # Creo la cuarta capa oculta, con las mismas características.\n",
        "    Dense(64, activation='relu'),\n",
        "\n",
        "    # Creo la capa de salida, con una única neurona y sin activación.\n",
        "    # Esto es adecuado para este problema de regresión, donde la salida es un valor continuo.\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "rnghqeQvttW_"
      },
      "id": "rnghqeQvttW_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilación del modelo\n",
        "# Uso el optimizador Adam, que combina lo mejor de RMSprop y momentum, y se adapta bien a la mayoría de los problemas.\n",
        "# Establezco una tasa de aprendizaje pequeña (0.001) para que el modelo aprenda de forma más estable.\n",
        "# Como es un problema de regresión, uso 'mse' (error cuadrático medio) como función de pérdida.\n",
        "# También incluyo 'mse' como métrica para monitorizar el rendimiento durante el entrenamiento y evaluación.\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),  # Optimizador Adam con tasa de aprendizaje 0.001\n",
        "    loss='mse',                           # Función de pérdida: mean squared error (regresión)\n",
        "    metrics=['mse']                       # Métrica: también el MSE, para visualizar el error\n",
        ")"
      ],
      "metadata": {
        "id": "Im1tqt2pvysU"
      },
      "id": "Im1tqt2pvysU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rotary-credits",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rotary-credits",
        "outputId": "f97e2d5f-4345-4f74-875a-7dbc5a99fd90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 14.6076 - mse: 14.6076 - val_loss: 1.7840 - val_mse: 1.7840\n",
            "Epoch 2/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3199 - mse: 1.3199 - val_loss: 1.0235 - val_mse: 1.0235\n",
            "Epoch 3/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8264 - mse: 0.8264 - val_loss: 0.6824 - val_mse: 0.6824\n",
            "Epoch 4/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6067 - mse: 0.6067 - val_loss: 0.5796 - val_mse: 0.5796\n",
            "Epoch 5/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5271 - mse: 0.5271 - val_loss: 0.5400 - val_mse: 0.5400\n",
            "Epoch 6/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4931 - mse: 0.4931 - val_loss: 0.5257 - val_mse: 0.5257\n",
            "Epoch 7/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4729 - mse: 0.4729 - val_loss: 0.5171 - val_mse: 0.5171\n",
            "Epoch 8/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4598 - mse: 0.4598 - val_loss: 0.5110 - val_mse: 0.5110\n",
            "Epoch 9/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4486 - mse: 0.4486 - val_loss: 0.5073 - val_mse: 0.5073\n",
            "Epoch 10/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.5026 - val_mse: 0.5026\n",
            "Epoch 11/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4330 - mse: 0.4330 - val_loss: 0.5002 - val_mse: 0.5002\n",
            "Epoch 12/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4249 - mse: 0.4249 - val_loss: 0.4979 - val_mse: 0.4979\n",
            "Epoch 13/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4178 - mse: 0.4178 - val_loss: 0.4997 - val_mse: 0.4997\n",
            "Epoch 14/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4120 - mse: 0.4120 - val_loss: 0.4981 - val_mse: 0.4981\n",
            "Epoch 15/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4067 - mse: 0.4067 - val_loss: 0.4989 - val_mse: 0.4989\n",
            "Epoch 16/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3998 - mse: 0.3998 - val_loss: 0.5003 - val_mse: 0.5003\n",
            "Epoch 17/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3942 - mse: 0.3942 - val_loss: 0.4989 - val_mse: 0.4989\n",
            "Epoch 18/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3876 - mse: 0.3876 - val_loss: 0.5002 - val_mse: 0.5002\n",
            "Epoch 19/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3826 - mse: 0.3826 - val_loss: 0.5011 - val_mse: 0.5011\n",
            "Epoch 20/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3773 - mse: 0.3773 - val_loss: 0.5040 - val_mse: 0.5040\n",
            "Epoch 21/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3716 - mse: 0.3716 - val_loss: 0.5060 - val_mse: 0.5060\n",
            "Epoch 22/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3685 - mse: 0.3685 - val_loss: 0.5046 - val_mse: 0.5046\n",
            "Epoch 23/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3630 - mse: 0.3630 - val_loss: 0.5054 - val_mse: 0.5054\n",
            "Epoch 24/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3581 - mse: 0.3581 - val_loss: 0.5058 - val_mse: 0.5058\n",
            "Epoch 25/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3533 - mse: 0.3533 - val_loss: 0.5140 - val_mse: 0.5140\n",
            "Epoch 26/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3509 - mse: 0.3509 - val_loss: 0.5192 - val_mse: 0.5192\n",
            "Epoch 27/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3467 - mse: 0.3467 - val_loss: 0.5222 - val_mse: 0.5222\n",
            "Epoch 28/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3426 - mse: 0.3426 - val_loss: 0.5209 - val_mse: 0.5209\n",
            "Epoch 29/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3362 - mse: 0.3362 - val_loss: 0.5170 - val_mse: 0.5170\n",
            "Epoch 30/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3337 - mse: 0.3337 - val_loss: 0.5295 - val_mse: 0.5295\n",
            "Epoch 31/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3296 - mse: 0.3296 - val_loss: 0.5209 - val_mse: 0.5209\n",
            "Epoch 32/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3242 - mse: 0.3242 - val_loss: 0.5214 - val_mse: 0.5214\n",
            "Epoch 33/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3199 - mse: 0.3199 - val_loss: 0.5288 - val_mse: 0.5288\n",
            "Epoch 34/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3173 - mse: 0.3173 - val_loss: 0.5292 - val_mse: 0.5292\n",
            "Epoch 35/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3124 - mse: 0.3124 - val_loss: 0.5376 - val_mse: 0.5376\n",
            "Epoch 36/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3115 - mse: 0.3115 - val_loss: 0.5348 - val_mse: 0.5348\n",
            "Epoch 37/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3075 - mse: 0.3075 - val_loss: 0.5351 - val_mse: 0.5351\n",
            "Epoch 38/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3028 - mse: 0.3028 - val_loss: 0.5387 - val_mse: 0.5387\n",
            "Epoch 39/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2981 - mse: 0.2981 - val_loss: 0.5368 - val_mse: 0.5368\n",
            "Epoch 40/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2938 - mse: 0.2938 - val_loss: 0.5414 - val_mse: 0.5414\n",
            "Epoch 41/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2904 - mse: 0.2904 - val_loss: 0.5447 - val_mse: 0.5447\n",
            "Epoch 42/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2869 - mse: 0.2869 - val_loss: 0.5455 - val_mse: 0.5455\n",
            "Epoch 43/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2815 - mse: 0.2815 - val_loss: 0.5466 - val_mse: 0.5466\n",
            "Epoch 44/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2774 - mse: 0.2774 - val_loss: 0.5543 - val_mse: 0.5543\n",
            "Epoch 45/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2730 - mse: 0.2730 - val_loss: 0.5541 - val_mse: 0.5541\n",
            "Epoch 46/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2721 - mse: 0.2721 - val_loss: 0.5757 - val_mse: 0.5757\n",
            "Epoch 47/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2709 - mse: 0.2709 - val_loss: 0.5590 - val_mse: 0.5590\n",
            "Epoch 48/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2641 - mse: 0.2641 - val_loss: 0.5715 - val_mse: 0.5715\n",
            "Epoch 49/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.5656 - val_mse: 0.5656\n",
            "Epoch 50/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2549 - mse: 0.2549 - val_loss: 0.5678 - val_mse: 0.5678\n",
            "Epoch 51/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2520 - mse: 0.2520 - val_loss: 0.5701 - val_mse: 0.5701\n",
            "Epoch 52/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2497 - mse: 0.2497 - val_loss: 0.5634 - val_mse: 0.5634\n",
            "Epoch 53/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2440 - mse: 0.2440 - val_loss: 0.5715 - val_mse: 0.5715\n",
            "Epoch 54/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2446 - mse: 0.2446 - val_loss: 0.5677 - val_mse: 0.5677\n",
            "Epoch 55/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2376 - mse: 0.2376 - val_loss: 0.5756 - val_mse: 0.5756\n",
            "Epoch 56/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2372 - mse: 0.2372 - val_loss: 0.5628 - val_mse: 0.5628\n",
            "Epoch 57/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2331 - mse: 0.2331 - val_loss: 0.5649 - val_mse: 0.5649\n",
            "Epoch 58/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2271 - mse: 0.2271 - val_loss: 0.5713 - val_mse: 0.5713\n",
            "Epoch 59/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2229 - mse: 0.2229 - val_loss: 0.5697 - val_mse: 0.5697\n",
            "Epoch 60/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2193 - mse: 0.2193 - val_loss: 0.5696 - val_mse: 0.5696\n",
            "Epoch 61/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2182 - mse: 0.2182 - val_loss: 0.5710 - val_mse: 0.5710\n",
            "Epoch 62/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2184 - mse: 0.2184 - val_loss: 0.5782 - val_mse: 0.5782\n",
            "Epoch 63/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2168 - mse: 0.2168 - val_loss: 0.5756 - val_mse: 0.5756\n",
            "Epoch 64/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2113 - mse: 0.2113 - val_loss: 0.5822 - val_mse: 0.5822\n",
            "Epoch 65/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2110 - mse: 0.2110 - val_loss: 0.5789 - val_mse: 0.5789\n",
            "Epoch 66/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2065 - mse: 0.2065 - val_loss: 0.5909 - val_mse: 0.5909\n",
            "Epoch 67/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2079 - mse: 0.2079 - val_loss: 0.5971 - val_mse: 0.5971\n",
            "Epoch 68/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2040 - mse: 0.2040 - val_loss: 0.6031 - val_mse: 0.6031\n",
            "Epoch 69/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2017 - mse: 0.2017 - val_loss: 0.5962 - val_mse: 0.5962\n",
            "Epoch 70/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1980 - mse: 0.1980 - val_loss: 0.5941 - val_mse: 0.5941\n",
            "Epoch 71/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1948 - mse: 0.1948 - val_loss: 0.5944 - val_mse: 0.5944\n",
            "Epoch 72/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1945 - mse: 0.1945 - val_loss: 0.6046 - val_mse: 0.6046\n",
            "Epoch 73/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1918 - mse: 0.1918 - val_loss: 0.6024 - val_mse: 0.6024\n",
            "Epoch 74/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1902 - mse: 0.1902 - val_loss: 0.6168 - val_mse: 0.6168\n",
            "Epoch 75/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1861 - mse: 0.1861 - val_loss: 0.6175 - val_mse: 0.6175\n",
            "Epoch 76/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1876 - mse: 0.1876 - val_loss: 0.6120 - val_mse: 0.6120\n",
            "Epoch 77/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1905 - mse: 0.1905 - val_loss: 0.6226 - val_mse: 0.6226\n",
            "Epoch 78/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1882 - mse: 0.1882 - val_loss: 0.6182 - val_mse: 0.6182\n",
            "Epoch 79/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1901 - mse: 0.1901 - val_loss: 0.6350 - val_mse: 0.6350\n",
            "Epoch 80/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1924 - mse: 0.1924 - val_loss: 0.6536 - val_mse: 0.6536\n",
            "Epoch 81/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1985 - mse: 0.1985 - val_loss: 0.6463 - val_mse: 0.6463\n",
            "Epoch 82/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2065 - mse: 0.2065 - val_loss: 0.6420 - val_mse: 0.6420\n",
            "Epoch 83/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2022 - mse: 0.2022 - val_loss: 0.6408 - val_mse: 0.6408\n",
            "Epoch 84/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1887 - mse: 0.1887 - val_loss: 0.6315 - val_mse: 0.6315\n",
            "Epoch 85/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1793 - mse: 0.1793 - val_loss: 0.6094 - val_mse: 0.6094\n",
            "Epoch 86/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1761 - mse: 0.1761 - val_loss: 0.6251 - val_mse: 0.6251\n",
            "Epoch 87/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1769 - mse: 0.1769 - val_loss: 0.6487 - val_mse: 0.6487\n",
            "Epoch 88/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1747 - mse: 0.1747 - val_loss: 0.6688 - val_mse: 0.6688\n",
            "Epoch 89/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1731 - mse: 0.1731 - val_loss: 0.6795 - val_mse: 0.6795\n",
            "Epoch 90/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1692 - mse: 0.1692 - val_loss: 0.7127 - val_mse: 0.7127\n",
            "Epoch 91/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1693 - mse: 0.1693 - val_loss: 0.6921 - val_mse: 0.6921\n",
            "Epoch 92/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1644 - mse: 0.1644 - val_loss: 0.6914 - val_mse: 0.6914\n",
            "Epoch 93/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1600 - mse: 0.1600 - val_loss: 0.6762 - val_mse: 0.6762\n",
            "Epoch 94/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1530 - mse: 0.1530 - val_loss: 0.6744 - val_mse: 0.6744\n",
            "Epoch 95/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1455 - mse: 0.1455 - val_loss: 0.6753 - val_mse: 0.6753\n",
            "Epoch 96/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1461 - mse: 0.1461 - val_loss: 0.6627 - val_mse: 0.6627\n",
            "Epoch 97/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1436 - mse: 0.1436 - val_loss: 0.6576 - val_mse: 0.6576\n",
            "Epoch 98/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1385 - mse: 0.1385 - val_loss: 0.6620 - val_mse: 0.6620\n",
            "Epoch 99/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1340 - mse: 0.1340 - val_loss: 0.6600 - val_mse: 0.6600\n",
            "Epoch 100/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1331 - mse: 0.1331 - val_loss: 0.6566 - val_mse: 0.6566\n",
            "Epoch 101/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1306 - mse: 0.1306 - val_loss: 0.6636 - val_mse: 0.6636\n",
            "Epoch 102/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1285 - mse: 0.1285 - val_loss: 0.6591 - val_mse: 0.6591\n",
            "Epoch 103/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1262 - mse: 0.1262 - val_loss: 0.6784 - val_mse: 0.6784\n",
            "Epoch 104/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1257 - mse: 0.1257 - val_loss: 0.6737 - val_mse: 0.6737\n",
            "Epoch 105/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1259 - mse: 0.1259 - val_loss: 0.7019 - val_mse: 0.7019\n",
            "Epoch 106/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1270 - mse: 0.1270 - val_loss: 0.6981 - val_mse: 0.6981\n",
            "Epoch 107/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1258 - mse: 0.1258 - val_loss: 0.7497 - val_mse: 0.7497\n",
            "Epoch 108/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1242 - mse: 0.1242 - val_loss: 0.6915 - val_mse: 0.6915\n",
            "Epoch 109/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1276 - mse: 0.1276 - val_loss: 0.7191 - val_mse: 0.7191\n",
            "Epoch 110/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1209 - mse: 0.1209 - val_loss: 0.7480 - val_mse: 0.7480\n",
            "Epoch 111/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1273 - mse: 0.1273 - val_loss: 0.7307 - val_mse: 0.7307\n",
            "Epoch 112/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1218 - mse: 0.1218 - val_loss: 0.6832 - val_mse: 0.6832\n",
            "Epoch 113/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1244 - mse: 0.1244 - val_loss: 0.6846 - val_mse: 0.6846\n",
            "Epoch 114/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1172 - mse: 0.1172 - val_loss: 0.6924 - val_mse: 0.6924\n",
            "Epoch 115/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1159 - mse: 0.1159 - val_loss: 0.7002 - val_mse: 0.7002\n",
            "Epoch 116/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1143 - mse: 0.1143 - val_loss: 0.6776 - val_mse: 0.6776\n",
            "Epoch 117/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1222 - mse: 0.1222 - val_loss: 0.6697 - val_mse: 0.6697\n",
            "Epoch 118/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1274 - mse: 0.1274 - val_loss: 0.6838 - val_mse: 0.6838\n",
            "Epoch 119/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1375 - mse: 0.1375 - val_loss: 0.6872 - val_mse: 0.6872\n",
            "Epoch 120/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1436 - mse: 0.1436 - val_loss: 0.6568 - val_mse: 0.6568\n",
            "Epoch 121/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1537 - mse: 0.1537 - val_loss: 0.7151 - val_mse: 0.7151\n",
            "Epoch 122/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1494 - mse: 0.1494 - val_loss: 0.8196 - val_mse: 0.8196\n",
            "Epoch 123/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1460 - mse: 0.1460 - val_loss: 0.8369 - val_mse: 0.8369\n",
            "Epoch 124/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1500 - mse: 0.1500 - val_loss: 0.8240 - val_mse: 0.8240\n",
            "Epoch 125/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1538 - mse: 0.1538 - val_loss: 0.7546 - val_mse: 0.7546\n",
            "Epoch 126/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1669 - mse: 0.1669 - val_loss: 0.7048 - val_mse: 0.7048\n",
            "Epoch 127/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1884 - mse: 0.1884 - val_loss: 0.6243 - val_mse: 0.6243\n",
            "Epoch 128/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2255 - mse: 0.2255 - val_loss: 0.6745 - val_mse: 0.6745\n",
            "Epoch 129/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2122 - mse: 0.2122 - val_loss: 0.6588 - val_mse: 0.6588\n",
            "Epoch 130/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1734 - mse: 0.1734 - val_loss: 0.6537 - val_mse: 0.6537\n",
            "Epoch 131/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1442 - mse: 0.1442 - val_loss: 0.6381 - val_mse: 0.6381\n",
            "Epoch 132/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1274 - mse: 0.1274 - val_loss: 0.6517 - val_mse: 0.6517\n",
            "Epoch 133/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1183 - mse: 0.1183 - val_loss: 0.6490 - val_mse: 0.6490\n",
            "Epoch 134/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1079 - mse: 0.1079 - val_loss: 0.6551 - val_mse: 0.6551\n",
            "Epoch 135/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1029 - mse: 0.1029 - val_loss: 0.6571 - val_mse: 0.6571\n",
            "Epoch 136/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.6593 - val_mse: 0.6593\n",
            "Epoch 137/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0947 - mse: 0.0947 - val_loss: 0.6702 - val_mse: 0.6702\n",
            "Epoch 138/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0920 - mse: 0.0920 - val_loss: 0.6574 - val_mse: 0.6574\n",
            "Epoch 139/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0892 - mse: 0.0892 - val_loss: 0.6756 - val_mse: 0.6756\n",
            "Epoch 140/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.6737 - val_mse: 0.6737\n",
            "Epoch 141/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.6658 - val_mse: 0.6658\n",
            "Epoch 142/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0854 - mse: 0.0854 - val_loss: 0.6814 - val_mse: 0.6814\n",
            "Epoch 143/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0865 - mse: 0.0865 - val_loss: 0.6885 - val_mse: 0.6885\n",
            "Epoch 144/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.6835 - val_mse: 0.6835\n",
            "Epoch 145/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0916 - mse: 0.0916 - val_loss: 0.6898 - val_mse: 0.6898\n",
            "Epoch 146/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0862 - mse: 0.0862 - val_loss: 0.6844 - val_mse: 0.6844\n",
            "Epoch 147/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.7049 - val_mse: 0.7049\n",
            "Epoch 148/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0871 - mse: 0.0871 - val_loss: 0.6892 - val_mse: 0.6892\n",
            "Epoch 149/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0828 - mse: 0.0828 - val_loss: 0.7390 - val_mse: 0.7390\n",
            "Epoch 150/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0841 - mse: 0.0841 - val_loss: 0.7305 - val_mse: 0.7305\n",
            "Epoch 151/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0863 - mse: 0.0863 - val_loss: 0.6910 - val_mse: 0.6910\n",
            "Epoch 152/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0837 - mse: 0.0837 - val_loss: 0.7254 - val_mse: 0.7254\n",
            "Epoch 153/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0850 - mse: 0.0850 - val_loss: 0.7012 - val_mse: 0.7012\n",
            "Epoch 154/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0793 - mse: 0.0793 - val_loss: 0.7331 - val_mse: 0.7331\n",
            "Epoch 155/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0812 - mse: 0.0812 - val_loss: 0.7196 - val_mse: 0.7196\n",
            "Epoch 156/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0843 - mse: 0.0843 - val_loss: 0.6845 - val_mse: 0.6845\n",
            "Epoch 157/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0861 - mse: 0.0861 - val_loss: 0.7372 - val_mse: 0.7372\n",
            "Epoch 158/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0856 - mse: 0.0856 - val_loss: 0.7135 - val_mse: 0.7135\n",
            "Epoch 159/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0773 - mse: 0.0773 - val_loss: 0.7447 - val_mse: 0.7447\n",
            "Epoch 160/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0854 - mse: 0.0854 - val_loss: 0.7260 - val_mse: 0.7260\n",
            "Epoch 161/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.7117 - val_mse: 0.7117\n",
            "Epoch 162/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0795 - mse: 0.0795 - val_loss: 0.7463 - val_mse: 0.7463\n",
            "Epoch 163/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0914 - mse: 0.0914 - val_loss: 0.7818 - val_mse: 0.7818\n",
            "Epoch 164/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0895 - mse: 0.0895 - val_loss: 0.8053 - val_mse: 0.8053\n",
            "Epoch 165/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0968 - mse: 0.0968 - val_loss: 0.8429 - val_mse: 0.8429\n",
            "Epoch 166/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1111 - mse: 0.1111 - val_loss: 0.7997 - val_mse: 0.7997\n",
            "Epoch 167/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1200 - mse: 0.1200 - val_loss: 0.7116 - val_mse: 0.7116\n",
            "Epoch 168/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1294 - mse: 0.1294 - val_loss: 0.6853 - val_mse: 0.6853\n",
            "Epoch 169/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1399 - mse: 0.1399 - val_loss: 0.7340 - val_mse: 0.7340\n",
            "Epoch 170/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1417 - mse: 0.1417 - val_loss: 0.7619 - val_mse: 0.7619\n",
            "Epoch 171/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1329 - mse: 0.1329 - val_loss: 0.7436 - val_mse: 0.7436\n",
            "Epoch 172/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1334 - mse: 0.1334 - val_loss: 0.7608 - val_mse: 0.7608\n",
            "Epoch 173/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1289 - mse: 0.1289 - val_loss: 0.7244 - val_mse: 0.7244\n",
            "Epoch 174/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1163 - mse: 0.1163 - val_loss: 0.6967 - val_mse: 0.6967\n",
            "Epoch 175/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1029 - mse: 0.1029 - val_loss: 0.6924 - val_mse: 0.6924\n",
            "Epoch 176/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0945 - mse: 0.0945 - val_loss: 0.6904 - val_mse: 0.6904\n",
            "Epoch 177/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0843 - mse: 0.0843 - val_loss: 0.6704 - val_mse: 0.6704\n",
            "Epoch 178/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0788 - mse: 0.0788 - val_loss: 0.6672 - val_mse: 0.6672\n",
            "Epoch 179/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0748 - mse: 0.0748 - val_loss: 0.6691 - val_mse: 0.6691\n",
            "Epoch 180/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0716 - mse: 0.0716 - val_loss: 0.6844 - val_mse: 0.6844\n",
            "Epoch 181/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0699 - mse: 0.0699 - val_loss: 0.6768 - val_mse: 0.6768\n",
            "Epoch 182/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0676 - mse: 0.0676 - val_loss: 0.6754 - val_mse: 0.6754\n",
            "Epoch 183/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0679 - mse: 0.0679 - val_loss: 0.6822 - val_mse: 0.6822\n",
            "Epoch 184/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0678 - mse: 0.0678 - val_loss: 0.6802 - val_mse: 0.6802\n",
            "Epoch 185/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0675 - mse: 0.0675 - val_loss: 0.6854 - val_mse: 0.6854\n",
            "Epoch 186/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0687 - mse: 0.0687 - val_loss: 0.6763 - val_mse: 0.6763\n",
            "Epoch 187/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0681 - mse: 0.0681 - val_loss: 0.6928 - val_mse: 0.6928\n",
            "Epoch 188/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0686 - mse: 0.0686 - val_loss: 0.6820 - val_mse: 0.6820\n",
            "Epoch 189/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0725 - mse: 0.0725 - val_loss: 0.6787 - val_mse: 0.6787\n",
            "Epoch 190/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0705 - mse: 0.0705 - val_loss: 0.6964 - val_mse: 0.6964\n",
            "Epoch 191/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0779 - mse: 0.0779 - val_loss: 0.6943 - val_mse: 0.6943\n",
            "Epoch 192/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0728 - mse: 0.0728 - val_loss: 0.6513 - val_mse: 0.6513\n",
            "Epoch 193/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0790 - mse: 0.0790 - val_loss: 0.6756 - val_mse: 0.6756\n",
            "Epoch 194/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0793 - mse: 0.0793 - val_loss: 0.6963 - val_mse: 0.6963\n",
            "Epoch 195/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0766 - mse: 0.0766 - val_loss: 0.7026 - val_mse: 0.7026\n",
            "Epoch 196/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0875 - mse: 0.0875 - val_loss: 0.6864 - val_mse: 0.6864\n",
            "Epoch 197/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0783 - mse: 0.0783 - val_loss: 0.7085 - val_mse: 0.7085\n",
            "Epoch 198/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0845 - mse: 0.0845 - val_loss: 0.7262 - val_mse: 0.7262\n",
            "Epoch 199/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0865 - mse: 0.0865 - val_loss: 0.7318 - val_mse: 0.7318\n",
            "Epoch 200/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0863 - mse: 0.0863 - val_loss: 0.7771 - val_mse: 0.7771\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7acd10378310>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# No modifico el código\n",
        "# Se entrena el modelo con los datos de entrenamiento\n",
        "# Se usan 200 épocas, lo que indica que el modelo verá el conjunto completo de entrenamiento 200 veces.\n",
        "# batch_size=32 significa que el gradiente se actualiza cada 32 muestras.\n",
        "# validation_split=0.2 reserva el 20% del conjunto de entrenamiento para validación.\n",
        "# verbose=1 muestra una barra de progreso por cada época.\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=200,\n",
        "          batch_size=32,\n",
        "          validation_split=0.2,\n",
        "          verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "descending-letters",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "descending-letters",
        "outputId": "2a17dd77-cf00-4a8e-dedf-33a7ae70c475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7064 - mse: 0.7064\n",
            "Test Loss: [0.7462330460548401, 0.7462330460548401]\n"
          ]
        }
      ],
      "source": [
        "# No modifico el código.\n",
        "# Se evalúa el modelo utilizando el conjunto de test (datos no vistos durante el entrenamiento).\n",
        "# Esto permite obtener una estimación objetiva del rendimiento del modelo en datos nuevos.\n",
        "# verbose=1 muestra el progreso del proceso de evaluación.\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "# Se imprime el valor de la pérdida (loss) en el conjunto de test.\n",
        "# En este caso, la métrica corresponde al error cuadrático medio (MSE).\n",
        "print('Test Loss: {}'.format(results))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explicación de lo realizado\n",
        "\n",
        "En esta primera cuestión construí un modelo secuencial con cuatro capas ocultas, cada una con 64 neuronas y la función de activación `ReLU`, tal como se especificaba en el enunciado. No añadí ninguna técnica de regularización, ya que el objetivo era trabajar con una arquitectura base sin restricciones adicionales.\n",
        "\n",
        "La capa de salida contiene una única neurona sin función de activación, lo cual es apropiado para problemas de regresión, donde se espera que el modelo devuelva un valor continuo.\n",
        "\n",
        "Compilé el modelo utilizando el optimizador `Adam` con una tasa de aprendizaje de 0.001, y como función de pérdida elegí el error cuadrático medio (`mse`), que es la más habitual y adecuada para tareas de regresión.\n",
        "\n",
        "Entrené la red durante 200 épocas, con un 20% de los datos de entrenamiento reservado para validación (`validation_split=0.2`). Esto me permitió observar cómo evolucionaba la pérdida en los datos de validación y verificar que el modelo aprendía correctamente.\n",
        "\n",
        "Finalmente, evalué el modelo sobre el conjunto de test y se imprimió el `Test Loss`, tal como solicitaba la consigna.\n",
        "\n",
        "**Nota:** Aunque fijé la semilla de aleatoriedad con `np.random.seed(42)` y `tf.random.set_seed(42)`, es posible que los resultados varíen ligeramente entre ejecuciones, sobre todo si se usa GPU. Esto se debe a que algunas operaciones de TensorFlow no son completamente deterministas en entornos paralelos.\n"
      ],
      "metadata": {
        "id": "y1YugKBtDZIW"
      },
      "id": "y1YugKBtDZIW"
    },
    {
      "cell_type": "markdown",
      "id": "raised-delivery",
      "metadata": {
        "id": "raised-delivery"
      },
      "source": [
        "<a name='1.2'></a>\n",
        "## Cuestión 2: Utilice el mismo modelo de la cuestión anterior pero añadiendo al menos dos técnicas distinas de regularización. No es necesario reducir el test loss.\n",
        "\n",
        "Ejemplos de regularización: [Prevent_Overfitting.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/Fundamentals/Prevent_Overfitting.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Fijo la semilla para NumPy, para asegurar resultados reproducibles.\n",
        "np.random.seed(42)\n",
        "\n",
        "# Fijo la semilla para TensorFlow, para asegurar que los resultados del entrenamiento sean consistentes.\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Defino el modelo secuencial con regularización L2 y una capa de entrada explícita.\n",
        "model = Sequential([\n",
        "    Input(shape=(x_train.shape[1],)),  # Capa de entrada que se adapta al número de características del dataset.\n",
        "\n",
        "    # Primera capa oculta\n",
        "    # Aplico regularización L2 para penalizar pesos grandes (técnica 1)\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "\n",
        "    # Aplico Batch Normalization para estabilizar el entrenamiento (técnica 2)\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Aplico Dropout para evitar sobreajuste (25% de neuronas desactivadas aleatoriamente) (técnica 3)\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # Segunda capa oculta con las tres técnicas también\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),  # L2\n",
        "    BatchNormalization(),                                        # BatchNormalization\n",
        "    Dropout(0.25),                                               # Dropout\n",
        "\n",
        "    # Tercera capa oculta\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),  # L2\n",
        "    BatchNormalization(),                                        # BatchNormalization\n",
        "    # Aquí no uso Dropout, pero sigue habiendo regularización L2 + BatchNorm\n",
        "\n",
        "    # Cuarta capa oculta\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),  # Solo regularización L2\n",
        "\n",
        "    # Capa de salida: una sola neurona sin activación (regresión)\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "u2pU5X9LSLMO"
      },
      "id": "u2pU5X9LSLMO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "focal-traffic",
      "metadata": {
        "id": "focal-traffic"
      },
      "outputs": [],
      "source": [
        "# Compilación del modelo.\n",
        "\n",
        "# Uso el optimizador Adam con una tasa de aprendizaje pequeña (0.001),\n",
        "# que suele funcionar bien en muchos problemas de deep learning.\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "# Compilo el modelo usando como función de pérdida el error cuadrático medio (MSE),\n",
        "# ya que se trata de un problema de regresión.\n",
        "# También añado el MSE como métrica para monitorizar durante el entrenamiento.\n",
        "model.compile(\n",
        "    optimizer=optimizer,  # Optimizador Adam\n",
        "    loss='mse',           # Función de pérdida: error cuadrático medio\n",
        "    metrics=['mse']       # Métrica usada para evaluar el desempeño del modelo\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "338f8622",
      "metadata": {
        "id": "338f8622"
      },
      "outputs": [],
      "source": [
        "# Defino el tamaño del batch (lote) que se usará en cada paso de entrenamiento.\n",
        "# Uso un batch_size de 32, ya que es un valor común, que ofrece un buen equilibrio entre velocidad de entrenamiento y estabilidad.\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prostate-instrumentation",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prostate-instrumentation",
        "outputId": "79503e82-d334-4b0a-9b16-374bd8c415e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 27.0415 - mse: 26.8322 - val_loss: 4.6031 - val_mse: 4.3877\n",
            "Epoch 2/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5268 - mse: 1.3113 - val_loss: 0.9640 - val_mse: 0.7487\n",
            "Epoch 3/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0872 - mse: 0.8721 - val_loss: 0.8206 - val_mse: 0.6062\n",
            "Epoch 4/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9870 - mse: 0.7730 - val_loss: 0.8316 - val_mse: 0.6185\n",
            "Epoch 5/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9511 - mse: 0.7384 - val_loss: 0.8030 - val_mse: 0.5913\n",
            "Epoch 6/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9167 - mse: 0.7055 - val_loss: 0.7859 - val_mse: 0.5759\n",
            "Epoch 7/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9143 - mse: 0.7047 - val_loss: 0.7741 - val_mse: 0.5658\n",
            "Epoch 8/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8571 - mse: 0.6493 - val_loss: 0.7495 - val_mse: 0.5429\n",
            "Epoch 9/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8409 - mse: 0.6349 - val_loss: 0.7491 - val_mse: 0.5444\n",
            "Epoch 10/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8294 - mse: 0.6253 - val_loss: 0.7440 - val_mse: 0.5414\n",
            "Epoch 11/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8273 - mse: 0.6252 - val_loss: 0.7238 - val_mse: 0.5233\n",
            "Epoch 12/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7979 - mse: 0.5979 - val_loss: 0.7168 - val_mse: 0.5182\n",
            "Epoch 13/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7897 - mse: 0.5917 - val_loss: 0.7248 - val_mse: 0.5286\n",
            "Epoch 14/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7617 - mse: 0.5660 - val_loss: 0.7062 - val_mse: 0.5122\n",
            "Epoch 15/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7779 - mse: 0.5845 - val_loss: 0.6969 - val_mse: 0.5051\n",
            "Epoch 16/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7389 - mse: 0.5476 - val_loss: 0.7016 - val_mse: 0.5122\n",
            "Epoch 17/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7589 - mse: 0.5701 - val_loss: 0.7183 - val_mse: 0.5312\n",
            "Epoch 18/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7539 - mse: 0.5674 - val_loss: 0.6922 - val_mse: 0.5075\n",
            "Epoch 19/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7343 - mse: 0.5502 - val_loss: 0.6846 - val_mse: 0.5025\n",
            "Epoch 20/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7340 - mse: 0.5525 - val_loss: 0.6763 - val_mse: 0.4966\n",
            "Epoch 21/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7356 - mse: 0.5565 - val_loss: 0.6946 - val_mse: 0.5173\n",
            "Epoch 22/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7249 - mse: 0.5482 - val_loss: 0.6711 - val_mse: 0.4965\n",
            "Epoch 23/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7304 - mse: 0.5563 - val_loss: 0.6667 - val_mse: 0.4946\n",
            "Epoch 24/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6993 - mse: 0.5278 - val_loss: 0.6630 - val_mse: 0.4934\n",
            "Epoch 25/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7035 - mse: 0.5345 - val_loss: 0.6584 - val_mse: 0.4912\n",
            "Epoch 26/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7041 - mse: 0.5376 - val_loss: 0.6372 - val_mse: 0.4727\n",
            "Epoch 27/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6827 - mse: 0.5187 - val_loss: 0.6378 - val_mse: 0.4758\n",
            "Epoch 28/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6708 - mse: 0.5094 - val_loss: 0.6512 - val_mse: 0.4918\n",
            "Epoch 29/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6707 - mse: 0.5118 - val_loss: 0.6471 - val_mse: 0.4902\n",
            "Epoch 30/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6814 - mse: 0.5250 - val_loss: 0.6362 - val_mse: 0.4817\n",
            "Epoch 31/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6739 - mse: 0.5200 - val_loss: 0.6468 - val_mse: 0.4947\n",
            "Epoch 32/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6680 - mse: 0.5165 - val_loss: 0.6259 - val_mse: 0.4763\n",
            "Epoch 33/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6442 - mse: 0.4953 - val_loss: 0.6283 - val_mse: 0.4813\n",
            "Epoch 34/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6537 - mse: 0.5072 - val_loss: 0.6463 - val_mse: 0.5017\n",
            "Epoch 35/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6511 - mse: 0.5070 - val_loss: 0.6149 - val_mse: 0.4726\n",
            "Epoch 36/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6356 - mse: 0.4939 - val_loss: 0.6238 - val_mse: 0.4836\n",
            "Epoch 37/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6438 - mse: 0.5041 - val_loss: 0.5996 - val_mse: 0.4617\n",
            "Epoch 38/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6322 - mse: 0.4949 - val_loss: 0.5990 - val_mse: 0.4632\n",
            "Epoch 39/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6233 - mse: 0.4881 - val_loss: 0.6057 - val_mse: 0.4721\n",
            "Epoch 40/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6170 - mse: 0.4838 - val_loss: 0.6000 - val_mse: 0.4685\n",
            "Epoch 41/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6166 - mse: 0.4856 - val_loss: 0.6134 - val_mse: 0.4838\n",
            "Epoch 42/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6229 - mse: 0.4939 - val_loss: 0.5960 - val_mse: 0.4684\n",
            "Epoch 43/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5930 - mse: 0.4659 - val_loss: 0.5976 - val_mse: 0.4721\n",
            "Epoch 44/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6045 - mse: 0.4794 - val_loss: 0.5895 - val_mse: 0.4659\n",
            "Epoch 45/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5976 - mse: 0.4744 - val_loss: 0.5988 - val_mse: 0.4771\n",
            "Epoch 46/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5998 - mse: 0.4785 - val_loss: 0.5907 - val_mse: 0.4709\n",
            "Epoch 47/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6007 - mse: 0.4813 - val_loss: 0.5970 - val_mse: 0.4790\n",
            "Epoch 48/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5912 - mse: 0.4737 - val_loss: 0.5761 - val_mse: 0.4600\n",
            "Epoch 49/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5736 - mse: 0.4579 - val_loss: 0.5824 - val_mse: 0.4681\n",
            "Epoch 50/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5667 - mse: 0.4527 - val_loss: 0.5813 - val_mse: 0.4685\n",
            "Epoch 51/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5775 - mse: 0.4650 - val_loss: 0.5928 - val_mse: 0.4815\n",
            "Epoch 52/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5452 - mse: 0.4342 - val_loss: 0.5736 - val_mse: 0.4638\n",
            "Epoch 53/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5707 - mse: 0.4613 - val_loss: 0.5850 - val_mse: 0.4767\n",
            "Epoch 54/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5732 - mse: 0.4651 - val_loss: 0.5708 - val_mse: 0.4639\n",
            "Epoch 55/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5573 - mse: 0.4507 - val_loss: 0.5772 - val_mse: 0.4717\n",
            "Epoch 56/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5713 - mse: 0.4660 - val_loss: 0.5908 - val_mse: 0.4866\n",
            "Epoch 57/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5565 - mse: 0.4526 - val_loss: 0.5819 - val_mse: 0.4790\n",
            "Epoch 58/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5689 - mse: 0.4663 - val_loss: 0.5730 - val_mse: 0.4714\n",
            "Epoch 59/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5355 - mse: 0.4341 - val_loss: 0.5682 - val_mse: 0.4678\n",
            "Epoch 60/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5471 - mse: 0.4470 - val_loss: 0.5670 - val_mse: 0.4678\n",
            "Epoch 61/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5441 - mse: 0.4452 - val_loss: 0.5794 - val_mse: 0.4814\n",
            "Epoch 62/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5325 - mse: 0.4348 - val_loss: 0.5679 - val_mse: 0.4711\n",
            "Epoch 63/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5359 - mse: 0.4394 - val_loss: 0.5671 - val_mse: 0.4714\n",
            "Epoch 64/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5385 - mse: 0.4430 - val_loss: 0.5733 - val_mse: 0.4785\n",
            "Epoch 65/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5263 - mse: 0.4317 - val_loss: 0.5566 - val_mse: 0.4630\n",
            "Epoch 66/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5285 - mse: 0.4351 - val_loss: 0.5663 - val_mse: 0.4736\n",
            "Epoch 67/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5345 - mse: 0.4420 - val_loss: 0.5627 - val_mse: 0.4710\n",
            "Epoch 68/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5456 - mse: 0.4542 - val_loss: 0.5681 - val_mse: 0.4774\n",
            "Epoch 69/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5263 - mse: 0.4358 - val_loss: 0.5483 - val_mse: 0.4585\n",
            "Epoch 70/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5230 - mse: 0.4334 - val_loss: 0.5480 - val_mse: 0.4590\n",
            "Epoch 71/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5355 - mse: 0.4468 - val_loss: 0.5510 - val_mse: 0.4629\n",
            "Epoch 72/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5273 - mse: 0.4394 - val_loss: 0.5587 - val_mse: 0.4714\n",
            "Epoch 73/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5045 - mse: 0.4173 - val_loss: 0.5745 - val_mse: 0.4880\n",
            "Epoch 74/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5148 - mse: 0.4284 - val_loss: 0.5570 - val_mse: 0.4713\n",
            "Epoch 75/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5237 - mse: 0.4381 - val_loss: 0.5487 - val_mse: 0.4638\n",
            "Epoch 76/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5176 - mse: 0.4328 - val_loss: 0.5580 - val_mse: 0.4736\n",
            "Epoch 77/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5264 - mse: 0.4422 - val_loss: 0.5503 - val_mse: 0.4666\n",
            "Epoch 78/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5203 - mse: 0.4367 - val_loss: 0.5517 - val_mse: 0.4686\n",
            "Epoch 79/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5113 - mse: 0.4283 - val_loss: 0.5348 - val_mse: 0.4523\n",
            "Epoch 80/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4949 - mse: 0.4127 - val_loss: 0.5433 - val_mse: 0.4615\n",
            "Epoch 81/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5171 - mse: 0.4353 - val_loss: 0.5600 - val_mse: 0.4786\n",
            "Epoch 82/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5021 - mse: 0.4208 - val_loss: 0.5357 - val_mse: 0.4549\n",
            "Epoch 83/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5037 - mse: 0.4231 - val_loss: 0.5391 - val_mse: 0.4589\n",
            "Epoch 84/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5135 - mse: 0.4334 - val_loss: 0.5497 - val_mse: 0.4701\n",
            "Epoch 85/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5000 - mse: 0.4206 - val_loss: 0.5470 - val_mse: 0.4680\n",
            "Epoch 86/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4966 - mse: 0.4177 - val_loss: 0.5392 - val_mse: 0.4606\n",
            "Epoch 87/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4911 - mse: 0.4126 - val_loss: 0.5326 - val_mse: 0.4545\n",
            "Epoch 88/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4995 - mse: 0.4215 - val_loss: 0.5454 - val_mse: 0.4678\n",
            "Epoch 89/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4929 - mse: 0.4154 - val_loss: 0.5519 - val_mse: 0.4748\n",
            "Epoch 90/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4961 - mse: 0.4190 - val_loss: 0.5319 - val_mse: 0.4552\n",
            "Epoch 91/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4828 - mse: 0.4062 - val_loss: 0.5444 - val_mse: 0.4682\n",
            "Epoch 92/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4929 - mse: 0.4168 - val_loss: 0.5396 - val_mse: 0.4636\n",
            "Epoch 93/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4923 - mse: 0.4164 - val_loss: 0.5365 - val_mse: 0.4609\n",
            "Epoch 94/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4974 - mse: 0.4219 - val_loss: 0.5466 - val_mse: 0.4714\n",
            "Epoch 95/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5116 - mse: 0.4365 - val_loss: 0.5431 - val_mse: 0.4684\n",
            "Epoch 96/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4938 - mse: 0.4192 - val_loss: 0.5497 - val_mse: 0.4754\n",
            "Epoch 97/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4967 - mse: 0.4225 - val_loss: 0.5409 - val_mse: 0.4670\n",
            "Epoch 98/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4899 - mse: 0.4160 - val_loss: 0.5361 - val_mse: 0.4624\n",
            "Epoch 99/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4889 - mse: 0.4153 - val_loss: 0.5318 - val_mse: 0.4584\n",
            "Epoch 100/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4884 - mse: 0.4151 - val_loss: 0.5422 - val_mse: 0.4691\n",
            "Epoch 101/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4932 - mse: 0.4202 - val_loss: 0.5570 - val_mse: 0.4842\n",
            "Epoch 102/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4962 - mse: 0.4235 - val_loss: 0.5489 - val_mse: 0.4765\n",
            "Epoch 103/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4831 - mse: 0.4108 - val_loss: 0.5411 - val_mse: 0.4691\n",
            "Epoch 104/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4897 - mse: 0.4176 - val_loss: 0.5398 - val_mse: 0.4679\n",
            "Epoch 105/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4728 - mse: 0.4010 - val_loss: 0.5258 - val_mse: 0.4544\n",
            "Epoch 106/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4812 - mse: 0.4097 - val_loss: 0.5413 - val_mse: 0.4700\n",
            "Epoch 107/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4680 - mse: 0.3969 - val_loss: 0.5217 - val_mse: 0.4508\n",
            "Epoch 108/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4775 - mse: 0.4067 - val_loss: 0.5257 - val_mse: 0.4551\n",
            "Epoch 109/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4798 - mse: 0.4092 - val_loss: 0.5249 - val_mse: 0.4545\n",
            "Epoch 110/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4628 - mse: 0.3924 - val_loss: 0.5201 - val_mse: 0.4499\n",
            "Epoch 111/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4746 - mse: 0.4045 - val_loss: 0.5331 - val_mse: 0.4631\n",
            "Epoch 112/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4681 - mse: 0.3981 - val_loss: 0.5379 - val_mse: 0.4681\n",
            "Epoch 113/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4651 - mse: 0.3953 - val_loss: 0.5296 - val_mse: 0.4601\n",
            "Epoch 114/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4662 - mse: 0.3967 - val_loss: 0.5203 - val_mse: 0.4510\n",
            "Epoch 115/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4650 - mse: 0.3957 - val_loss: 0.5306 - val_mse: 0.4614\n",
            "Epoch 116/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4595 - mse: 0.3904 - val_loss: 0.5372 - val_mse: 0.4683\n",
            "Epoch 117/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4845 - mse: 0.4156 - val_loss: 0.5270 - val_mse: 0.4583\n",
            "Epoch 118/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4537 - mse: 0.3850 - val_loss: 0.5381 - val_mse: 0.4696\n",
            "Epoch 119/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4798 - mse: 0.4113 - val_loss: 0.5398 - val_mse: 0.4715\n",
            "Epoch 120/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4742 - mse: 0.4059 - val_loss: 0.5369 - val_mse: 0.4688\n",
            "Epoch 121/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4558 - mse: 0.3876 - val_loss: 0.5271 - val_mse: 0.4590\n",
            "Epoch 122/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4570 - mse: 0.3890 - val_loss: 0.5288 - val_mse: 0.4608\n",
            "Epoch 123/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4728 - mse: 0.4049 - val_loss: 0.5223 - val_mse: 0.4546\n",
            "Epoch 124/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4609 - mse: 0.3932 - val_loss: 0.5369 - val_mse: 0.4693\n",
            "Epoch 125/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4661 - mse: 0.3986 - val_loss: 0.5266 - val_mse: 0.4591\n",
            "Epoch 126/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4560 - mse: 0.3885 - val_loss: 0.5304 - val_mse: 0.4632\n",
            "Epoch 127/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4685 - mse: 0.4013 - val_loss: 0.5324 - val_mse: 0.4653\n",
            "Epoch 128/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4617 - mse: 0.3946 - val_loss: 0.5222 - val_mse: 0.4552\n",
            "Epoch 129/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4665 - mse: 0.3995 - val_loss: 0.5295 - val_mse: 0.4625\n",
            "Epoch 130/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4638 - mse: 0.3969 - val_loss: 0.5309 - val_mse: 0.4642\n",
            "Epoch 131/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4731 - mse: 0.4064 - val_loss: 0.5312 - val_mse: 0.4647\n",
            "Epoch 132/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4627 - mse: 0.3963 - val_loss: 0.5313 - val_mse: 0.4650\n",
            "Epoch 133/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4620 - mse: 0.3957 - val_loss: 0.5359 - val_mse: 0.4697\n",
            "Epoch 134/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4523 - mse: 0.3861 - val_loss: 0.5292 - val_mse: 0.4631\n",
            "Epoch 135/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4520 - mse: 0.3859 - val_loss: 0.5298 - val_mse: 0.4637\n",
            "Epoch 136/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4628 - mse: 0.3967 - val_loss: 0.5394 - val_mse: 0.4732\n",
            "Epoch 137/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4444 - mse: 0.3782 - val_loss: 0.5364 - val_mse: 0.4704\n",
            "Epoch 138/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4544 - mse: 0.3885 - val_loss: 0.5334 - val_mse: 0.4676\n",
            "Epoch 139/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4663 - mse: 0.4005 - val_loss: 0.5317 - val_mse: 0.4659\n",
            "Epoch 140/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4398 - mse: 0.3741 - val_loss: 0.5286 - val_mse: 0.4630\n",
            "Epoch 141/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4444 - mse: 0.3789 - val_loss: 0.5323 - val_mse: 0.4669\n",
            "Epoch 142/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4568 - mse: 0.3914 - val_loss: 0.5443 - val_mse: 0.4789\n",
            "Epoch 143/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4503 - mse: 0.3849 - val_loss: 0.5263 - val_mse: 0.4611\n",
            "Epoch 144/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4666 - mse: 0.4014 - val_loss: 0.5151 - val_mse: 0.4500\n",
            "Epoch 145/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4479 - mse: 0.3828 - val_loss: 0.5222 - val_mse: 0.4572\n",
            "Epoch 146/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4489 - mse: 0.3839 - val_loss: 0.5164 - val_mse: 0.4516\n",
            "Epoch 147/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4517 - mse: 0.3868 - val_loss: 0.5198 - val_mse: 0.4549\n",
            "Epoch 148/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4418 - mse: 0.3769 - val_loss: 0.5175 - val_mse: 0.4527\n",
            "Epoch 149/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4495 - mse: 0.3847 - val_loss: 0.5234 - val_mse: 0.4586\n",
            "Epoch 150/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4483 - mse: 0.3836 - val_loss: 0.5112 - val_mse: 0.4465\n",
            "Epoch 151/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4417 - mse: 0.3770 - val_loss: 0.5108 - val_mse: 0.4461\n",
            "Epoch 152/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4379 - mse: 0.3731 - val_loss: 0.5222 - val_mse: 0.4574\n",
            "Epoch 153/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4502 - mse: 0.3853 - val_loss: 0.5243 - val_mse: 0.4596\n",
            "Epoch 154/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4587 - mse: 0.3939 - val_loss: 0.5065 - val_mse: 0.4418\n",
            "Epoch 155/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4559 - mse: 0.3913 - val_loss: 0.5087 - val_mse: 0.4442\n",
            "Epoch 156/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4580 - mse: 0.3935 - val_loss: 0.5300 - val_mse: 0.4656\n",
            "Epoch 157/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4519 - mse: 0.3874 - val_loss: 0.5175 - val_mse: 0.4530\n",
            "Epoch 158/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4521 - mse: 0.3876 - val_loss: 0.5030 - val_mse: 0.4385\n",
            "Epoch 159/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4420 - mse: 0.3775 - val_loss: 0.5241 - val_mse: 0.4597\n",
            "Epoch 160/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4464 - mse: 0.3819 - val_loss: 0.5225 - val_mse: 0.4582\n",
            "Epoch 161/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4494 - mse: 0.3850 - val_loss: 0.5308 - val_mse: 0.4665\n",
            "Epoch 162/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4529 - mse: 0.3886 - val_loss: 0.5110 - val_mse: 0.4467\n",
            "Epoch 163/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4618 - mse: 0.3976 - val_loss: 0.5284 - val_mse: 0.4640\n",
            "Epoch 164/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4468 - mse: 0.3823 - val_loss: 0.5131 - val_mse: 0.4486\n",
            "Epoch 165/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4464 - mse: 0.3819 - val_loss: 0.5130 - val_mse: 0.4486\n",
            "Epoch 166/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4475 - mse: 0.3832 - val_loss: 0.5146 - val_mse: 0.4504\n",
            "Epoch 167/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4335 - mse: 0.3693 - val_loss: 0.5297 - val_mse: 0.4655\n",
            "Epoch 168/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4650 - mse: 0.4008 - val_loss: 0.5239 - val_mse: 0.4596\n",
            "Epoch 169/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4422 - mse: 0.3779 - val_loss: 0.5245 - val_mse: 0.4603\n",
            "Epoch 170/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4530 - mse: 0.3888 - val_loss: 0.5227 - val_mse: 0.4587\n",
            "Epoch 171/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4341 - mse: 0.3699 - val_loss: 0.5134 - val_mse: 0.4493\n",
            "Epoch 172/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4396 - mse: 0.3755 - val_loss: 0.5136 - val_mse: 0.4496\n",
            "Epoch 173/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4526 - mse: 0.3885 - val_loss: 0.5167 - val_mse: 0.4527\n",
            "Epoch 174/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4426 - mse: 0.3785 - val_loss: 0.5227 - val_mse: 0.4586\n",
            "Epoch 175/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4457 - mse: 0.3816 - val_loss: 0.5172 - val_mse: 0.4533\n",
            "Epoch 176/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4236 - mse: 0.3597 - val_loss: 0.5130 - val_mse: 0.4491\n",
            "Epoch 177/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4294 - mse: 0.3655 - val_loss: 0.5190 - val_mse: 0.4552\n",
            "Epoch 178/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4342 - mse: 0.3703 - val_loss: 0.5183 - val_mse: 0.4544\n",
            "Epoch 179/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4454 - mse: 0.3815 - val_loss: 0.5302 - val_mse: 0.4664\n",
            "Epoch 180/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4240 - mse: 0.3602 - val_loss: 0.5307 - val_mse: 0.4670\n",
            "Epoch 181/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4341 - mse: 0.3704 - val_loss: 0.5337 - val_mse: 0.4700\n",
            "Epoch 182/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4533 - mse: 0.3897 - val_loss: 0.5264 - val_mse: 0.4629\n",
            "Epoch 183/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4352 - mse: 0.3716 - val_loss: 0.5259 - val_mse: 0.4622\n",
            "Epoch 184/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4309 - mse: 0.3673 - val_loss: 0.5353 - val_mse: 0.4717\n",
            "Epoch 185/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4249 - mse: 0.3613 - val_loss: 0.5278 - val_mse: 0.4642\n",
            "Epoch 186/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4372 - mse: 0.3736 - val_loss: 0.5303 - val_mse: 0.4667\n",
            "Epoch 187/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4448 - mse: 0.3812 - val_loss: 0.5341 - val_mse: 0.4703\n",
            "Epoch 188/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4396 - mse: 0.3758 - val_loss: 0.5204 - val_mse: 0.4567\n",
            "Epoch 189/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4453 - mse: 0.3815 - val_loss: 0.5255 - val_mse: 0.4618\n",
            "Epoch 190/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4425 - mse: 0.3788 - val_loss: 0.5281 - val_mse: 0.4644\n",
            "Epoch 191/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4470 - mse: 0.3833 - val_loss: 0.5214 - val_mse: 0.4576\n",
            "Epoch 192/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4384 - mse: 0.3747 - val_loss: 0.5334 - val_mse: 0.4696\n",
            "Epoch 193/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4318 - mse: 0.3681 - val_loss: 0.5215 - val_mse: 0.4579\n",
            "Epoch 194/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4350 - mse: 0.3714 - val_loss: 0.5229 - val_mse: 0.4593\n",
            "Epoch 195/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4312 - mse: 0.3676 - val_loss: 0.5262 - val_mse: 0.4627\n",
            "Epoch 196/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4415 - mse: 0.3780 - val_loss: 0.5204 - val_mse: 0.4570\n",
            "Epoch 197/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4391 - mse: 0.3757 - val_loss: 0.5199 - val_mse: 0.4566\n",
            "Epoch 198/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4311 - mse: 0.3677 - val_loss: 0.5181 - val_mse: 0.4546\n",
            "Epoch 199/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4373 - mse: 0.3738 - val_loss: 0.5236 - val_mse: 0.4601\n",
            "Epoch 200/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4247 - mse: 0.3613 - val_loss: 0.5083 - val_mse: 0.4449\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7acd08105210>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# No modifico el código.\n",
        "# Se entrena el modelo usando los datos de entrenamiento (x_train, y_train).\n",
        "# El entrenamiento se realiza durante 200 épocas.\n",
        "# batch_size define cuántas muestras se procesan antes de actualizar los pesos.\n",
        "# validation_split=0.2 separa el 20% de los datos de entrenamiento para validación.\n",
        "# verbose=1 muestra el progreso del entrenamiento en pantalla.\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=200,\n",
        "          batch_size=batch_size,\n",
        "          validation_split=0.2,\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "friendly-powell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "friendly-powell",
        "outputId": "9a3901ab-e889-49d5-9602-e84583385741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5271 - mse: 0.4637\n",
            "Test Loss: [0.5438356399536133, 0.4804173409938812]\n"
          ]
        }
      ],
      "source": [
        "# No modifico el código.\n",
        "# Se evalúa el modelo con el conjunto de test (x_test, y_test),\n",
        "# para obtener el rendimiento final sobre datos no vistos durante el entrenamiento.\n",
        "\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "# Se Imprime la pérdida obtenida en el conjunto de test.\n",
        "# Lo que permite comprobar, el error del modelo una vez entrenado.\n",
        "print('Test Loss: {}'.format(results))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explicación de lo realizado\n",
        "\n",
        "En esta segunda cuestión utilicé la misma arquitectura del modelo que en la pregunta anterior: una red secuencial con 4 capas ocultas, cada una con 64 neuronas y activación ReLU, y una capa de salida con una sola neurona (sin activación), adecuada para regresión.\n",
        "\n",
        "Para cumplir con el objetivo de aplicar al menos **dos técnicas distintas de regularización**, incorporé las siguientes:\n",
        "\n",
        "- **Regularización L2** (`kernel_regularizer=l2(0.001)`) en todas las capas ocultas, para penalizar los pesos grandes y evitar que el modelo se sobreentrene.\n",
        "- **Dropout**, con una tasa del 25% después de las dos primeras capas ocultas, que desactiva aleatoriamente neuronas durante el entrenamiento y mejora la generalización.\n",
        "- **Batch Normalization**, después de cada capa oculta (excepto la última), para estabilizar y acelerar el aprendizaje ajustando la distribución de activaciones intermedias.\n",
        "\n",
        "Además, fijé una semilla con `np.random.seed(42)` y `tf.random.set_seed(42)` para mejorar la reproducibilidad.\n",
        "\n",
        "Compilé el modelo usando el optimizador **Adam** (por su buen rendimiento y ajuste automático de la tasa de aprendizaje) y la función de pérdida **MSE** (adecuada para tareas de regresión). Entrené la red durante 200 épocas con `batch_size = 32` y un `validation_split = 0.2`.\n",
        "\n",
        "Finalmente, evalué el modelo sobre el conjunto de test, tal como se indicaba en el enunciado.\n"
      ],
      "metadata": {
        "id": "E6B6ezSAD3Ia"
      },
      "id": "E6B6ezSAD3Ia"
    },
    {
      "cell_type": "markdown",
      "id": "british-vegetation",
      "metadata": {
        "id": "british-vegetation"
      },
      "source": [
        "<a name='1.3'></a>\n",
        "## Cuestión 3: Utilice el mismo modelo de la cuestión anterior pero añadiendo un callback de early stopping. No es necesario reducir el test loss."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Fijo la semilla para NumPy, para asegurar resultados reproducibles.\n",
        "np.random.seed(42)\n",
        "\n",
        "# Fijo la semilla para TensorFlow, para asegurar que los resultados del entrenamiento sean consistentes.\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Defino el modelo secuencial con regularización L2, normalización y una capa de entrada explícita.\n",
        "model = Sequential([\n",
        "    Input(shape=(x_train.shape[1],)),  # Capa de entrada que se adapta al número de características del dataset\n",
        "\n",
        "    # Primera capa oculta con 64 neuronas, activación ReLU y regularización L2\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),  # Normaliza las salidas para mejorar la estabilidad del entrenamiento\n",
        "    Dropout(0.25),         # Apaga aleatoriamente el 25% de las neuronas (regularización)\n",
        "\n",
        "    # Segunda capa oculta con las mismas características\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # Tercera capa oculta con normalización pero sin dropout\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Cuarta capa oculta solo con regularización L2\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "\n",
        "    # Capa de salida con una única neurona (sin activación, porque es un problema de regresión)\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "ePt7Js9eY6GY"
      },
      "id": "ePt7Js9eY6GY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "blond-telephone",
      "metadata": {
        "id": "blond-telephone"
      },
      "outputs": [],
      "source": [
        "# Compilación del modelo.\n",
        "# Utilizo el optimizador Adam con una tasa de aprendizaje de 0.001, adecuado para la mayoría de tareas de regresión.\n",
        "# Empleo la función de pérdida, que es el error cuadrático medio (MSE), ideal para problemas donde se predicen valores continuos.\n",
        "# También incluyo el MSE como métrica para monitorizar el rendimiento durante el entrenamiento.\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='mse',\n",
        "    metrics=['mse']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "subsequent-roads",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "subsequent-roads",
        "outputId": "1b5ab176-d972-4813-8322-e3826dc633a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 10.8430 - mse: 10.6244 - val_loss: 3.7275 - val_mse: 3.5035\n",
            "Epoch 2/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2122 - mse: 0.9884 - val_loss: 1.0323 - val_mse: 0.8093\n",
            "Epoch 3/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0476 - mse: 0.8250 - val_loss: 0.9419 - val_mse: 0.7204\n",
            "Epoch 4/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9867 - mse: 0.7657 - val_loss: 0.8452 - val_mse: 0.6256\n",
            "Epoch 5/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9245 - mse: 0.7053 - val_loss: 0.8308 - val_mse: 0.6131\n",
            "Epoch 6/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8839 - mse: 0.6668 - val_loss: 0.8218 - val_mse: 0.6062\n",
            "Epoch 7/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8863 - mse: 0.6712 - val_loss: 0.8259 - val_mse: 0.6125\n",
            "Epoch 8/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8708 - mse: 0.6579 - val_loss: 0.7975 - val_mse: 0.5864\n",
            "Epoch 9/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8477 - mse: 0.6370 - val_loss: 0.7565 - val_mse: 0.5476\n",
            "Epoch 10/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8089 - mse: 0.6005 - val_loss: 0.7583 - val_mse: 0.5516\n",
            "Epoch 11/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8083 - mse: 0.6022 - val_loss: 0.7148 - val_mse: 0.5107\n",
            "Epoch 12/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8097 - mse: 0.6061 - val_loss: 0.7357 - val_mse: 0.5341\n",
            "Epoch 13/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7819 - mse: 0.5809 - val_loss: 0.7227 - val_mse: 0.5236\n",
            "Epoch 14/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7880 - mse: 0.5894 - val_loss: 0.7226 - val_mse: 0.5260\n",
            "Epoch 15/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7626 - mse: 0.5665 - val_loss: 0.7016 - val_mse: 0.5075\n",
            "Epoch 16/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7674 - mse: 0.5739 - val_loss: 0.6949 - val_mse: 0.5034\n",
            "Epoch 17/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7622 - mse: 0.5712 - val_loss: 0.6897 - val_mse: 0.5007\n",
            "Epoch 18/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7247 - mse: 0.5363 - val_loss: 0.7044 - val_mse: 0.5181\n",
            "Epoch 19/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7272 - mse: 0.5415 - val_loss: 0.6886 - val_mse: 0.5050\n",
            "Epoch 20/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7172 - mse: 0.5342 - val_loss: 0.6799 - val_mse: 0.4990\n",
            "Epoch 21/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7136 - mse: 0.5332 - val_loss: 0.6758 - val_mse: 0.4976\n",
            "Epoch 22/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7179 - mse: 0.5402 - val_loss: 0.6755 - val_mse: 0.4999\n",
            "Epoch 23/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7154 - mse: 0.5404 - val_loss: 0.6768 - val_mse: 0.5039\n",
            "Epoch 24/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6925 - mse: 0.5201 - val_loss: 0.6712 - val_mse: 0.5009\n",
            "Epoch 25/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7034 - mse: 0.5337 - val_loss: 0.6707 - val_mse: 0.5032\n",
            "Epoch 26/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6838 - mse: 0.5168 - val_loss: 0.6620 - val_mse: 0.4970\n",
            "Epoch 27/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6732 - mse: 0.5087 - val_loss: 0.6589 - val_mse: 0.4964\n",
            "Epoch 28/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6647 - mse: 0.5027 - val_loss: 0.6504 - val_mse: 0.4905\n",
            "Epoch 29/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6515 - mse: 0.4922 - val_loss: 0.6591 - val_mse: 0.5017\n",
            "Epoch 30/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6616 - mse: 0.5048 - val_loss: 0.6439 - val_mse: 0.4890\n",
            "Epoch 31/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6461 - mse: 0.4917 - val_loss: 0.6432 - val_mse: 0.4909\n",
            "Epoch 32/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6400 - mse: 0.4883 - val_loss: 0.6442 - val_mse: 0.4944\n",
            "Epoch 33/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6307 - mse: 0.4814 - val_loss: 0.6568 - val_mse: 0.5094\n",
            "Epoch 34/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6358 - mse: 0.4890 - val_loss: 0.6424 - val_mse: 0.4975\n",
            "Epoch 35/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6122 - mse: 0.4678 - val_loss: 0.6461 - val_mse: 0.5036\n",
            "Epoch 36/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6153 - mse: 0.4732 - val_loss: 0.6412 - val_mse: 0.5010\n",
            "Epoch 37/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6152 - mse: 0.4754 - val_loss: 0.6328 - val_mse: 0.4947\n",
            "Epoch 38/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6072 - mse: 0.4696 - val_loss: 0.6275 - val_mse: 0.4916\n",
            "Epoch 39/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6036 - mse: 0.4682 - val_loss: 0.6376 - val_mse: 0.5038\n",
            "Epoch 40/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6046 - mse: 0.4713 - val_loss: 0.6131 - val_mse: 0.4814\n",
            "Epoch 41/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6034 - mse: 0.4721 - val_loss: 0.6247 - val_mse: 0.4951\n",
            "Epoch 42/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5928 - mse: 0.4635 - val_loss: 0.6144 - val_mse: 0.4866\n",
            "Epoch 43/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5741 - mse: 0.4468 - val_loss: 0.6164 - val_mse: 0.4905\n",
            "Epoch 44/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6098 - mse: 0.4842 - val_loss: 0.6138 - val_mse: 0.4897\n",
            "Epoch 45/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5849 - mse: 0.4613 - val_loss: 0.6178 - val_mse: 0.4956\n",
            "Epoch 46/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5879 - mse: 0.4660 - val_loss: 0.6137 - val_mse: 0.4933\n",
            "Epoch 47/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5837 - mse: 0.4636 - val_loss: 0.6070 - val_mse: 0.4884\n",
            "Epoch 48/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5766 - mse: 0.4582 - val_loss: 0.6070 - val_mse: 0.4900\n",
            "Epoch 49/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5733 - mse: 0.4566 - val_loss: 0.5938 - val_mse: 0.4785\n",
            "Epoch 50/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5580 - mse: 0.4430 - val_loss: 0.5847 - val_mse: 0.4711\n",
            "Epoch 51/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5755 - mse: 0.4622 - val_loss: 0.5933 - val_mse: 0.4812\n",
            "Epoch 52/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5653 - mse: 0.4535 - val_loss: 0.5921 - val_mse: 0.4815\n",
            "Epoch 53/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5580 - mse: 0.4477 - val_loss: 0.5866 - val_mse: 0.4776\n",
            "Epoch 54/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5412 - mse: 0.4324 - val_loss: 0.5881 - val_mse: 0.4805\n",
            "Epoch 55/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5458 - mse: 0.4385 - val_loss: 0.5854 - val_mse: 0.4793\n",
            "Epoch 56/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5477 - mse: 0.4417 - val_loss: 0.5853 - val_mse: 0.4805\n",
            "Epoch 57/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5566 - mse: 0.4519 - val_loss: 0.5864 - val_mse: 0.4828\n",
            "Epoch 58/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5520 - mse: 0.4486 - val_loss: 0.5900 - val_mse: 0.4876\n",
            "Epoch 59/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5358 - mse: 0.4336 - val_loss: 0.5777 - val_mse: 0.4766\n",
            "Epoch 60/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5412 - mse: 0.4402 - val_loss: 0.5803 - val_mse: 0.4803\n",
            "Epoch 61/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5199 - mse: 0.4201 - val_loss: 0.5991 - val_mse: 0.5002\n",
            "Epoch 62/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5283 - mse: 0.4295 - val_loss: 0.5774 - val_mse: 0.4796\n",
            "Epoch 63/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5303 - mse: 0.4326 - val_loss: 0.5840 - val_mse: 0.4872\n",
            "Epoch 64/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5316 - mse: 0.4350 - val_loss: 0.5801 - val_mse: 0.4843\n",
            "Epoch 65/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5405 - mse: 0.4449 - val_loss: 0.5787 - val_mse: 0.4839\n",
            "Epoch 66/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5162 - mse: 0.4216 - val_loss: 0.5857 - val_mse: 0.4919\n",
            "Epoch 67/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5374 - mse: 0.4437 - val_loss: 0.5765 - val_mse: 0.4836\n",
            "Epoch 68/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5130 - mse: 0.4203 - val_loss: 0.5674 - val_mse: 0.4754\n",
            "Epoch 69/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5153 - mse: 0.4236 - val_loss: 0.5782 - val_mse: 0.4872\n",
            "Epoch 70/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5223 - mse: 0.4315 - val_loss: 0.5790 - val_mse: 0.4888\n",
            "Epoch 71/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5102 - mse: 0.4201 - val_loss: 0.5827 - val_mse: 0.4933\n",
            "Epoch 72/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5324 - mse: 0.4432 - val_loss: 0.5745 - val_mse: 0.4859\n",
            "Epoch 73/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5127 - mse: 0.4241 - val_loss: 0.5735 - val_mse: 0.4856\n",
            "Epoch 74/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5117 - mse: 0.4239 - val_loss: 0.6024 - val_mse: 0.5151\n",
            "Epoch 75/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5048 - mse: 0.4177 - val_loss: 0.5782 - val_mse: 0.4916\n",
            "Epoch 76/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5110 - mse: 0.4245 - val_loss: 0.5749 - val_mse: 0.4890\n",
            "Epoch 77/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5030 - mse: 0.4171 - val_loss: 0.5729 - val_mse: 0.4877\n",
            "Epoch 78/200\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5132 - mse: 0.4280 - val_loss: 0.5810 - val_mse: 0.4962\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7acce1b7bf10>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "## defino el early stopping callback.\n",
        "# Uso EarlyStopping, para evitar entrenar más de lo necesario si no mejora la validación.\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',         # miro la pérdida en validación.\n",
        "    patience=10,                # espero 10 épocas sin mejora.\n",
        "    restore_best_weights=True   # recupero los mejores pesos.\n",
        ")\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=200,\n",
        "          batch_size=32,\n",
        "          validation_split=0.2,\n",
        "          verbose=1,\n",
        "          callbacks=[early_stop])  # aplico el callback de early stopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pressing-object",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pressing-object",
        "outputId": "9788fe11-1f38-49ce-b936-f1adf245e818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5829 - mse: 0.4910\n",
            "Test Loss: [0.6117718815803528, 0.5198562741279602]\n"
          ]
        }
      ],
      "source": [
        "# No modifico el código.\n",
        "# Se evalúa el modelo entrenado sobre el conjunto de test.\n",
        "# Esto calcula la pérdida (loss) y las métricas especificadas durante la compilación (en este caso, el MSE).\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "# Se imprime el resultado de la evaluación (pérdida y métrica) en formato legible.\n",
        "print('Test Loss: {}'.format(results))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explicación de lo realizado\n",
        "\n",
        "En esta cuestión partí del mismo modelo usado en la Cuestión 2, que ya incluía diversas técnicas de regularización: regularización **L2**, **Dropout** y **BatchNormalization**, todas aplicadas para reducir el riesgo de sobreajuste en una red profunda.\n",
        "\n",
        "Además, introduje una novedad importante: el uso del **callback de EarlyStopping**. Esta técnica permite detener el entrenamiento de forma anticipada cuando la pérdida en el conjunto de validación (`val_loss`) deja de mejorar. Esto ayuda a evitar el sobreentrenamiento y a reducir el tiempo de entrenamiento innecesario.\n",
        "\n",
        "El callback se configuró con:\n",
        "- `monitor='val_loss'`: se observa la pérdida en validación,\n",
        "- `patience=10`: se permite un margen de 10 épocas sin mejora antes de detener el entrenamiento,\n",
        "- `restore_best_weights=True`: se restauran automáticamente los mejores pesos alcanzados.\n",
        "\n",
        "Como en las cuestiones anteriores:\n",
        "- Fijé una **semilla para NumPy (`np.random.seed(42)`)** y otra para **TensorFlow (`tf.random.set_seed(42)`)**. Esto busca asegurar resultados más consistentes y reproducibles entre ejecuciones.\n",
        "- Utilicé el optimizador **Adam** con tasa de aprendizaje 0.001.\n",
        "- La función de pérdida fue el **error cuadrático medio (`mse`)**, adecuada para problemas de regresión.\n",
        "- Entrené durante un máximo de 200 épocas, aunque el EarlyStopping detuvo el proceso antes de tiempo al no detectarse mejoras.\n",
        "\n",
        "Finalmente, evalué el modelo con el conjunto de test, imprimiendo la métrica final (`Test Loss`) tal como se pedía en el enunciado."
      ],
      "metadata": {
        "id": "BXKXZs9Hd-XJ"
      },
      "id": "BXKXZs9Hd-XJ"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}